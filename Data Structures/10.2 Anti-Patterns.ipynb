{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c3ff5b2-ef05-4912-a887-d3392bbd60ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1) Nested loops used as a join (quadratic join)**\n",
    "\n",
    "**Problem example (real)**: you have users (100k dicts) and orders (1M dicts). You want to attach a user's profile to each order by matching user_id. A naive nested loop checks every user for every order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b2c6ee1-113a-4211-aed8-c699112ddc1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Bad approach (conceptual):\n",
    "for order in orders:\n",
    "    for user in users:\n",
    "        if order['user_id'] == user['user_id']:\n",
    "            order['user'] = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea4561e0-b299-44a8-b465-00b83bca2147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why this hurts:** it compares every pair — with n = len(orders) and m = len(users) you get O(n * m) comparisons. This explodes quickly: 100k × 1M is impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3fb0e8d-0f7e-46b4-a09a-515f078e72e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Better (hash-based lookup):\n",
    "users_by_id = {u['user_id']: u for u in users}          # O(m) time, O(m) extra space\n",
    "for order in orders:                                    # O(n)\n",
    "    user = users_by_id.get(order['user_id'])\n",
    "    if user is not None:\n",
    "        order['user'] = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "507f471f-ce49-4c3f-837c-e1fe08749b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Complexities**\n",
    "\n",
    "Bad (nested loops):\n",
    "\n",
    "Time: O(n * m)\n",
    "\n",
    "Extra space: O(1) (ignoring input storage)\n",
    "\n",
    "Good (hash join):\n",
    "\n",
    "Time: O(n + m) amortized\n",
    "\n",
    "Extra space: O(m) for the index\n",
    "\n",
    "**Trade-offs / notes**\n",
    "\n",
    "Hash join uses extra memory for the dictionary. If m is huge and memory constrained, consider:\n",
    "\n",
    "Sort both lists on id and do a merge-join (O(n log n + m log m) for sort, O(n + m) for merge) with lower extra RAM.\n",
    "\n",
    "Use disk-backed joins (SQLite, pandas on-disk, or Spark) for very large datasets.\n",
    "\n",
    "If keys are not unique (one-to-many), store lists in the index: {id: [rows...]}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3598ef6b-0981-4e13-b756-d12552428da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2) Using a list for membership testing**\n",
    "\n",
    "**Problem example (real):** deduplicating incoming event IDs in a high-throughput loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e2f9db-0dae-401a-b862-d8b1cbbbc974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Bad approach:\n",
    "seen = []\n",
    "for eid in event_stream:\n",
    "    if eid not in seen:     # linear search across seen\n",
    "        seen.append(eid)\n",
    "        handle(eid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69d6a954-262f-45f4-b52c-372b5bec3716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why this hurts:** in on a list is O(k) where k is the list length. If you process n events and many are unique, overall time becomes O(n²) in the worst case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c4ddd0b-c551-40fa-b14c-cdcbecdee861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Better: use a set\n",
    "seen = set()\n",
    "for eid in event_stream:\n",
    "    if eid not in seen:     # avg O(1)\n",
    "        seen.add(eid)\n",
    "        handle(eid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1249c307-7bad-417a-a78f-e1902f12e84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Complexities**\n",
    "\n",
    "Bad (list membership):\n",
    "\n",
    "Time: O(n²) worst-case (or O(n * k) if there’s bounded distincts)\n",
    "\n",
    "Space: O(k) where k = number of distinct ids\n",
    "\n",
    "Good (set):\n",
    "\n",
    "Time: O(n) amortized (O(1) per membership & insert)\n",
    "\n",
    "Space: O(k) (hash table overhead)\n",
    "\n",
    "**Trade-offs / notes**\n",
    "\n",
    "Sets use more memory per item than lists (hash table overhead). If k is tiny, list might be fine for simplicity.\n",
    "\n",
    "If memory is critical and IDs are integers in a small range, consider a bitarray/bitmap to reduce space.\n",
    "\n",
    "If items are unhashable, use something else (e.g., frozenset of serializations or a custom key)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0587cff6-2652-4ed3-b784-f144d9009f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3) Repeated string concatenation inside a loop**\n",
    "\n",
    "**Problem example (real)**: assembling a huge log or CSV line-by-line by s += line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff40b926-57a4-48a0-b571-aa0cd4a9fa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Bad approach:\n",
    "out = \"\"\n",
    "for chunk in chunks:\n",
    "    out = out + chunk      # creates a new string each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b527185-f81b-4c63-9dfe-ddf93be65488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why this hurts:** Python strings are immutable — each + allocates a new string and copies contents. Repeatedly concatenating n chunks leads to O(n²) copying work in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b9dccba-7d83-4a99-b55c-130343774965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Better approaches : Collect pieces and join once:\n",
    "\n",
    "parts = []\n",
    "for chunk in chunks:\n",
    "    parts.append(chunk)\n",
    "out = \"\".join(parts)       # single pass to allocate final string\n",
    "\n",
    "## Or use io.StringIO (streamed construction) for many small writes:\n",
    "from io import StringIO\n",
    "buf = StringIO()\n",
    "for chunk in chunks:\n",
    "    buf.write(chunk)\n",
    "out = buf.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e4e0ad-0825-42ae-8396-594bf5f82319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Complexities**\n",
    "\n",
    "Bad (repeated +):\n",
    "\n",
    "Time: O(total_length * n) in pathological cases (commonly approximated as O(n²) where n is number of pieces)\n",
    "\n",
    "Space: O(total_length) but with lots of temporary allocations\n",
    "\n",
    "Good (join or StringIO):\n",
    "\n",
    "Time: O(total_length) (single pass)\n",
    "\n",
    "Space: O(total_length) with only one final allocation (plus small overhead)\n",
    "\n",
    "**Trade-offs / notes**\n",
    "\n",
    "For bytes, prefer bytearray or b\"\".join(...).\n",
    "\n",
    "If pieces are streamed and you cannot hold all parts, write directly to a file or stream (avoid building a huge in-memory string).\n",
    "\n",
    "join requires that you can store references to all pieces (the list) temporarily — if pieces count is huge but each is small, StringIO may be friendlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b23f63-2c01-4d13-973f-aace071fc6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4) Using exceptions for normal control-flow (heavy exception cost)**\n",
    "\n",
    "**Problem example (real):** processing thousands of records and using try/except to check key presence in a dict every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6230745-e184-4251-8ba3-d5a378f4dd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Bad approach:\n",
    "for r in rows:\n",
    "    try:\n",
    "        val = mydict[r['k']]\n",
    "        use(val)\n",
    "    except KeyError:\n",
    "        handle_missing(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a52ed277-1844-4687-b9e6-7436d83bb3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why this hurts:** exceptions are comparatively expensive to raise and catch. If missing keys are common, the interpreter pays a large overhead repeatedly. Also using exceptions as logic obscures intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49aecb03-12b4-4ed5-8ab7-beca92712bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Better approaches : If missing keys are expected often: use .get():\n",
    "\n",
    "val = mydict.get(r['k'])\n",
    "if val is None:\n",
    "    handle_missing(r)\n",
    "else:\n",
    "    use(val)\n",
    "\n",
    "## Or use defaultdict or setdefault if appropriate:\n",
    "from collections import defaultdict\n",
    "mydict = defaultdict(lambda: default_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72441817-884a-478b-84c6-fe39b9f74610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Complexities**\n",
    "\n",
    "Both methods do an average O(1) dict lookup.\n",
    "\n",
    "Bad (exceptions often): still O(1) lookup for present keys, but overhead of exception handling makes per-iteration constant factor much larger.\n",
    "\n",
    "Good (get): O(1) average with much smaller constant overhead.\n",
    "\n",
    "**Trade-offs / notes**\n",
    "\n",
    "If missing keys are rare and the common case is that the key exists, a try/except can be slightly faster than in + lookup (because it avoids a second hash lookup). In microbenchmarks the “ask for forgiveness” pattern sometimes wins — but only when misses are truly rare. Always benchmark if you think this matters.\n",
    "\n",
    "Keep code readable: prefer .get() when it expresses intent (check or provide default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5aaccd2-4046-4a8e-b050-887eb8c8281c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5) Doing heavy compute or blocking I/O per item instead of batching**\n",
    "\n",
    "**Problem example (real):** for each incoming record you call an external API or write to database immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e88fddd3-ee89-4d3c-a3ec-4f001c131a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Bad approach:\n",
    "\n",
    "for record in stream:\n",
    "    result = compute_expensive(record)   # CPU or I/O heavy\n",
    "    db.write(row=result)                 # individual DB writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a69b66b-b295-4750-8769-e3cd76837958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why this hurts:** performing heavy operations per item multiplies overheads (network round-trip, DB transaction cost, context switches). Throughput collapses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f52c75d-53e6-421f-842f-9d17bf1679ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Better (buffer & batch):\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 1000\n",
    "for record in stream:\n",
    "    batch.append(record)\n",
    "    if len(batch) >= BATCH_SIZE:\n",
    "        results = compute_batch(batch)        # vectorized / bulk operation\n",
    "        db.write_many(results)\n",
    "        batch.clear()\n",
    "\n",
    "# flush remaining\n",
    "if batch:\n",
    "    results = compute_batch(batch)\n",
    "    db.write_many(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da70dd75-ac28-4159-98ba-66607559cf9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Complexities**\n",
    "\n",
    "Bad (per-item expensive I/O):\n",
    "\n",
    "Time: O(n * (latency + processing_per_item)) — latency dominates\n",
    "\n",
    "Space: minimal (O(1) buffer)\n",
    "\n",
    "Good (batching):\n",
    "\n",
    "Time: O(n) but with much smaller constant — overheads amortized over BATCH_SIZE\n",
    "\n",
    "Space: O(BATCH_SIZE) extra memory for the buffer\n",
    "\n",
    "**Trade-offs / notes**\n",
    "\n",
    "Batching increases memory use proportional to batch_size. Tune batch size to fit memory constraints and optimal throughput (DB or API often have sweet spots).\n",
    "\n",
    "For CPU-bound work, vectorized operations (NumPy, pandas) or parallelism (multiprocessing) are better than naive loops.\n",
    "\n",
    "If latency per item matters (real-time constraints), you may need a small batch or use async pipelines that pipeline compute and I/O.\n",
    "\n",
    "Consider backpressure: if downstream is slower, use a bounded queue and apply flow control rather than unbounded buffering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f7f70bd-af87-49a3-9bf9-479b662eab14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Pattern                 |                         Bad (common) time |                 Bad space |                         Improved time | Improved space |\n",
    "| ----------------------- | ----------------------------------------: | ------------------------: | ------------------------------------: | -------------: |\n",
    "| Nested loops join       |                                    O(n·m) |                      O(1) |                       O(n + m) (hash) |           O(m) |\n",
    "| List membership         |                              O(n·k) worst |                      O(k) |                  O(n) amortized (set) |           O(k) |\n",
    "| Repeated string `+=`    |                             ~O(n²) copies | O(total) with temporaries |                              O(total) |       O(total) |\n",
    "| Exceptions for flow     | O(n) but high constant if many exceptions |                      O(1) |              O(n) with small constant |           O(1) |\n",
    "| Compute inside I/O loop |                            O(n * latency) |                      O(1) | O(n) with smaller constant (batching) |  O(batch_size) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de95805b-5739-4f31-b5d1-64151ba70fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10.2 Anti-Patterns",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
