{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94737479-2488-42a1-80f5-b37c70aa073b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Splits large datasets into smaller, manageable pieces for processing.\n",
    "\n",
    "Useful in ETL pipelines where memory or API limits exist.\n",
    "\n",
    "Enables batch processing, streaming, or parallelization of data.\n",
    "\n",
    "Avoids holding the entire dataset in memory at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85797fb1-6c8b-45d3-b828-505f913e4249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5e72a6-4da1-4dd5-b5ea-6931ef6074d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Process rows in batches of N using OFFSET + LIMIT\n",
    "SELECT *\n",
    "FROM transactions\n",
    "ORDER BY transaction_id\n",
    "LIMIT 1000 OFFSET 0;\n",
    "\n",
    "SELECT *\n",
    "FROM transactions\n",
    "ORDER BY transaction_id\n",
    "LIMIT 1000 OFFSET 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "863988e3-1525-44f3-849e-1cf6fb55042d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Break a large iterable into fixed-size pieces; process each piece independently to save memory and improve efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02baffba-9916-4b0c-92db-05f1740ca2fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d6aa21-7264-4e2f-b885-dd34a053af47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunked(iterable, size):\n",
    "    \"\"\"Yield successive chunks from iterable of given size.\"\"\"\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]\n",
    "\n",
    "# Usage\n",
    "for chunk in chunked(data, 1000):\n",
    "    process(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e20f2a7-18ad-4102-b961-dfedd0ce1d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df3cba25-8a7d-4db3-bdfb-ae79e191a35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = list(range(1, 11))  # [1,2,3,...,10]\n",
    "for chunk in chunked(data, 3):\n",
    "    print(chunk)\n",
    "[1, 2, 3]\n",
    "[4, 5, 6]\n",
    "[7, 8, 9]\n",
    "[10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72bd7d82-8c1f-41a7-8377-71b6e14d584b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Split a list of 50 log entries into chunks of 7.\n",
    "\n",
    "Process a CSV file in chunks of 500 rows using a generator.\n",
    "\n",
    "Chunk a string into pieces of 4 characters each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a35af4e4-4cc2-4632-8eef-f82accf59f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Scenario**\n",
    "\n",
    "Problem: A payments API allows only 100 transactions per request. You need to send 2,350 transactions.\n",
    "\n",
    "Expected Output: 24 API calls (23 full batches, 1 partial batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf0fd17-e675-4284-b82f-b8a5176e1064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def send_batches(transactions, batch_size=100):\n",
    "    for batch in chunked(transactions, batch_size):\n",
    "        api.send(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4f4788-4f5d-4afc-9448-0429c0a14a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Time Complexity: O(n) – every element is visited once.\n",
    "\n",
    "Space Complexity: O(k) – each chunk of size k is held in memory at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "773141e1-3ea4-48a7-99b8-b402e2e319ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls & Mistakes**\n",
    "\n",
    "❌ Loading the entire dataset in memory before chunking.\n",
    "❌ Modifying chunks in place without copying if the original dataset must remain intact.\n",
    "✔ Use generators to avoid high memory usage.\n",
    "✔ Ensure the last chunk may be smaller than the chunk size."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09. CHUNKING",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
