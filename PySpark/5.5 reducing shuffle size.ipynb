{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de86ef8-92ff-47a9-9c1b-0a86ca97e6eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Shuffles in Spark (caused by wide transformations like groupBy, join, distinct, repartition) are costly in time and network I/O.\n",
    "Reducing shuffle size improves job runtime, memory usage, and cluster efficiency.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "Aggregations over massive datasets.\n",
    "\n",
    "Joins where one table is much smaller.\n",
    "\n",
    "Optimizing ETL pipelines for speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b0dbc0-a2f4-4be5-a355-55edb1f16723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2968fe9-0338-415f-be52-87a5261d913a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Standard join (may shuffle all data)\n",
    "SELECT *\n",
    "FROM big_table b\n",
    "JOIN big_table2 b2\n",
    "ON b.id = b2.id;\n",
    "\n",
    "-- Reduce shuffle: pre-aggregate / filter\n",
    "WITH filtered_b AS (\n",
    "  SELECT id, SUM(amount) AS total\n",
    "  FROM big_table\n",
    "  WHERE date = '2025-12-01'\n",
    "  GROUP BY id\n",
    ")\n",
    "SELECT *\n",
    "FROM filtered_b fb\n",
    "JOIN small_table st\n",
    "ON fb.id = st.id;\n",
    "\n",
    "-- Pre-aggregation or filtering reduces the amount of data that moves during shuffle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "642ccb52-cbf2-4d96-97ba-eb2b7623df1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Shuffle = moving data across partitions → expensive.\n",
    "\n",
    "Reduce shuffle by:\n",
    "\n",
    "Pre-aggregate / filter large datasets.\n",
    "\n",
    "Broadcast small tables in joins.\n",
    "\n",
    "Use partitioning to colocate data.\n",
    "\n",
    "Avoid unnecessary repartition or wide transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f641727-af18-4677-b241-408f0986fb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2d2e99-cc2c-41d9-a8d0-58fc86b76df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, broadcast\n",
    "\n",
    "# Pre-filter / pre-aggregate\n",
    "df_small = df.filter(col(\"date\") == \"2025-12-01\") \\\n",
    "             .groupBy(\"id\").agg(spark_sum(\"amount\").alias(\"total\"))\n",
    "\n",
    "# Broadcast small table in join to avoid shuffle\n",
    "df_joined = df_small.join(broadcast(small_table), \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff79445f-89ea-4481-ad58-fc6444dec19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43124b6-f40f-441a-b0c4-739066280937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PerfPatterns\").getOrCreate()\n",
    "\n",
    "big_data = [(i, 100) for i in range(1000000)]\n",
    "small_data = [(i, \"NY\") for i in range(1000)]\n",
    "\n",
    "df_big = spark.createDataFrame(big_data, [\"id\", \"amount\"])\n",
    "df_small = spark.createDataFrame(small_data, [\"id\", \"state\"])\n",
    "\n",
    "# Pre-filter / pre-aggregate\n",
    "df_agg = df_big.filter(col(\"amount\") > 50).groupBy(\"id\").agg(spark_sum(\"amount\").alias(\"total\"))\n",
    "\n",
    "# Broadcast join reduces shuffle\n",
    "df_joined = df_agg.join(broadcast(df_small), \"id\")\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "092309b1-8e55-41d2-8fde-f56bb8b2cde5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Filtering before groupBy reduces rows shuffled.\n",
    "\n",
    "Broadcasting small table avoids full shuffle join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07710991-4f8e-47d2-a172-7bbfa5731c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Reduce shuffle when joining a 500M row table with a 1K row table.\n",
    "\n",
    "Explain why filtering early can improve shuffle performance.\n",
    "\n",
    "How would you partition a dataset to reduce shuffle in multiple joins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9890f58-5bbd-4d32-ac29-3a07701e9ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: You need to compute monthly revenue per customer from 1TB of transactional data and join it with a 10K customer profile table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b65667-c744-4d6f-9309-5d71f2c3d1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, broadcast\n",
    "\n",
    "# Step 1: Pre-filter data for December\n",
    "df_dec = transactions.filter(col(\"month\") == 12)\n",
    "\n",
    "# Step 2: Pre-aggregate revenue per customer\n",
    "df_revenue = df_dec.groupBy(\"customer_id\").agg(spark_sum(\"amount\").alias(\"monthly_revenue\"))\n",
    "\n",
    "# Step 3: Broadcast join with small customer profile\n",
    "df_final = df_revenue.join(broadcast(customer_profile), \"customer_id\")\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/silver/revenue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83289d2b-6980-4f93-906e-df4d3a504c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "| Operation                              | Time Complexity                     | Space Complexity      |\n",
    "| -------------------------------------- | ----------------------------------- | --------------------- |\n",
    "| Wide transformation w/ shuffle         | O(n log p + shuffle)                | O(n + shuffle buffer) |\n",
    "| Wide transformation w/ reduced shuffle | O(n_filtered log p + shuffle_small) | O(n_filtered)         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad805b5-a43f-4712-9367-3a2bd12690da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Shuffling full dataset unnecessarily (e.g., join before filtering).\n",
    "\n",
    "Not broadcasting small tables → unnecessary data movement.\n",
    "\n",
    "Ignoring partitioning → uneven shuffle.\n",
    "\n",
    "Pre-aggregating incorrectly → inaccurate results."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.5 reducing shuffle size",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
