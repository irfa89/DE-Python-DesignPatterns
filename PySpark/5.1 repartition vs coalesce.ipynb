{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85964a67-840d-46fb-8721-d5b633191234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Efficiently controls the number of partitions in a PySpark DataFrame to optimize parallelism and shuffle costs.\n",
    "\n",
    "repartition → increases or evenly redistributes partitions (full shuffle).\n",
    "\n",
    "coalesce → reduces partitions without full shuffle, faster for downsizing.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "Scaling out joins or aggregations (repartition).\n",
    "\n",
    "Writing large files to fewer partitions (coalesce).\n",
    "\n",
    "Optimizing memory and execution in distributed jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3b270dc-0815-457c-92e2-2388e3a9ca0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5c19272-21e2-4e53-bba3-8bbfc5f8e134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Repartition-like effect (force shuffle)\n",
    "CREATE TABLE new_table AS\n",
    "SELECT *\n",
    "FROM big_table\n",
    "DISTRIBUTE BY some_column;\n",
    "\n",
    "-- Coalesce-like effect (reduce partitions)\n",
    "-- Not explicit in SQL; achieved by controlling output file count:\n",
    "INSERT OVERWRITE TABLE small_table\n",
    "SELECT *\n",
    "FROM big_table\n",
    "CLUSTERED BY some_column INTO 4 BUCKETS;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a466473c-d752-4f3a-ba1b-fe466188ee9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Repartition: Full shuffle → evenly distributes data → increases parallelism.\n",
    "\n",
    "Coalesce: Avoids shuffle → reduces partitions → faster for downsizing.\n",
    "\n",
    "Always balance number of partitions vs data size for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c774f21-e8e7-4eec-a5ef-5cd8146356b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b38190f-cd19-47d2-80bb-67d77d7bb848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition: increase or redistribute partitions\n",
    "df_repart = df.repartition(num_partitions, \"column_name\")\n",
    "\n",
    "# Coalesce: reduce partitions without shuffle\n",
    "df_coal = df.coalesce(num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11acf283-0f43-4522-b73b-0ed1f758907d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16444d83-82d4-4503-82d4-3fbce5c91a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PerfPatterns\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 100), (\"Bob\", 200), (\"Charlie\", 300)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"score\"])\n",
    "\n",
    "# Original partitions\n",
    "print(df.rdd.getNumPartitions())  # e.g., 1\n",
    "\n",
    "# Increase partitions (shuffle)\n",
    "df_repart = df.repartition(4)\n",
    "print(df_repart.rdd.getNumPartitions())  # 4\n",
    "\n",
    "# Reduce partitions (no shuffle)\n",
    "df_coal = df_repart.coalesce(2)\n",
    "print(df_coal.rdd.getNumPartitions())  # 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c47474-b15d-491b-9e3b-5ad756b7bea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Repartition → forces shuffle → data spread across 4 partitions.\n",
    "\n",
    "Coalesce → merges partitions → avoids unnecessary shuffle → faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25164b6a-250d-455f-9e8a-6cc20989155b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Increase DataFrame df from 3 to 8 partitions using column \"score\".\n",
    "\n",
    "Reduce a 10-partition DataFrame to 3 partitions without shuffle.\n",
    "\n",
    "When writing a DataFrame to disk, how would you reduce the output files efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f849dda-6328-48f2-aa77-3a3955768bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: You have a 100GB transactional dataset in Bronze. You need to write it as 10 Parquet files in Silver while preparing for a join-heavy aggregation for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3daf1095-5c65-4ee4-aac0-3796c8b2ff3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Repartition for join-heavy operation\n",
    "df_silver = df_bronze.repartition(200, \"customer_id\")\n",
    "\n",
    "# Step 2: Aggregate analytics\n",
    "agg_df = df_silver.groupBy(\"customer_id\").sum(\"amount\")\n",
    "\n",
    "# Step 3: Coalesce for efficient write\n",
    "agg_df.coalesce(10).write.parquet(\"s3://silver/aggregates/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9071b78c-dc76-41e0-ba19-8a3d3b76e446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "| Operation   | Time Complexity     | Space Complexity                    |\n",
    "| ----------- | ------------------- | ----------------------------------- |\n",
    "| repartition | O(n) (full shuffle) | O(n) (all partitions may hold data) |\n",
    "| coalesce    | O(n/p) (no shuffle) | O(n) (merged partitions)            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4af874b-bc89-462a-9f24-70047207bc54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Using repartition unnecessarily → triggers costly shuffle.\n",
    "\n",
    "Using coalesce to increase partitions → doesn’t evenly distribute data.\n",
    "\n",
    "Ignoring partitioning when writing large datasets → small files problem.\n",
    "\n",
    "Forgetting cluster resources → too many partitions can degrade performance."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.1 repartition vs coalesce",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
