{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3eb7733-bb18-49bf-a114-fd52727a4f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Used when data must go through multiple transformations in a pipeline, e.g., cleaning → enriching → aggregating → writing. Instead of writing messy code in one block, we chain steps for readability, maintainability, and reusability.\n",
    "\n",
    "**Use-cases:**\n",
    "\n",
    "Bronze → Silver → Gold Delta pipelines\n",
    "\n",
    "Cleaning raw logs → Enriching → Aggregating → Storing\n",
    "\n",
    "Sequential feature engineering for ML pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c057a88b-f110-4fe9-853c-90d2e903751a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c23063c1-ada8-43ba-8e40-f58ea0878a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Step 1: Clean data\n",
    "WITH cleaned AS (\n",
    "    SELECT *, TRIM(name) AS clean_name FROM raw_table\n",
    "),\n",
    "-- Step 2: Enrich data\n",
    "enriched AS (\n",
    "    SELECT *, CONCAT(clean_name, '_2025') AS enriched_name FROM cleaned\n",
    ")\n",
    "-- Step 3: Aggregate data\n",
    "SELECT enriched_name, COUNT(*) AS cnt\n",
    "FROM enriched\n",
    "GROUP BY enriched_name;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca6835e4-0605-45d1-9cc1-68173c41b484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Transform → Transform → Transform in chained operations. In PySpark, use DataFrame transformations (withColumn, filter, join, groupBy) without immediate action until the last write or show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c11d405c-d28e-42f3-a032-3a54eeb3adb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbdda3fe-524d-4bc5-b00a-7d15af91e501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_raw.withColumn(\"col_clean\", <transformation>)\n",
    "df_enriched = df_cleaned.withColumn(\"col_enriched\", <transformation>)\n",
    "df_aggregated = df_enriched.groupBy(\"col_enriched\").agg(count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "# Write to sink\n",
    "df_aggregated.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58070c32-b784-4f35-a16b-6ee3138d9e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20dbc5ac-9521-47de-b867-8b9aacf215cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, concat, lit, count\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [(\" Alice \", 100), (\"Bob\", 200), (\"alice\", 50)]\n",
    "df_raw = spark.createDataFrame(data, [\"name\", \"amount\"])\n",
    "\n",
    "# Step 1: Clean\n",
    "df_cleaned = df_raw.withColumn(\"clean_name\", trim(col(\"name\")))\n",
    "\n",
    "# Step 2: Enrich\n",
    "df_enriched = df_cleaned.withColumn(\"enriched_name\", concat(col(\"clean_name\"), lit(\"_2025\")))\n",
    "\n",
    "# Step 3: Aggregate\n",
    "df_aggregated = df_enriched.groupBy(\"enriched_name\").agg(count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "df_aggregated.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c7ea478-337f-4a83-a9f5-e194fe296941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "+-------------+---+\n",
    "|enriched_name|cnt|\n",
    "+-------------+---+\n",
    "|Alice_2025   | 1 |\n",
    "|Bob_2025     | 1 |\n",
    "|alice_2025   | 1 |\n",
    "+-------------+---+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f0a3175-0fa3-49bd-8664-0fbe0d8e6b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Chain a filter → withColumn → groupBy on a dataset of sales transactions.\n",
    "\n",
    "Create a transformation chain that normalizes a city column and counts occurrences.\n",
    "\n",
    "Read a CSV → drop nulls → add a calculated column → write to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264a4353-db0e-4dae-8aa2-71f17fb65a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: You ingest clickstream logs (JSON). Build a pipeline:\n",
    "\n",
    "Extract relevant fields (user_id, url, timestamp).\n",
    "\n",
    "Clean url column (trim, lowercase).\n",
    "\n",
    "Enrich with domain column extracted from URL.\n",
    "\n",
    "Aggregate clicks per domain per day.\n",
    "\n",
    "Write to Delta Silver table partitioned by date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba519af0-0628-430d-9c51-1e856c69e0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Each transformation is lazy, so only DAG is built.\n",
    "\n",
    "groupBy and joins → shuffle, O(n log n) for sorting/shuffling depending on cluster.\n",
    "\n",
    "Memory usage depends on partitioning; better to repartition before heavy operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75270b3a-68b5-48bb-971e-caa914cf9f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Immediate actions in each step (e.g., collect()) → breaks pipeline and increases memory usage.\n",
    "\n",
    "Not reusing column objects, leading to messy code.\n",
    "\n",
    "Ignoring partitioning, causing shuffle bottlenecks on aggregations.\n",
    "\n",
    "Hardcoding paths instead of parameterized output."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7.1 multistep transformation chain",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
