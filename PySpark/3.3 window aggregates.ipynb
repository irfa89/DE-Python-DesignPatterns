{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50543f49-cbad-462b-8fac-dc8fbcbb715b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Enables row-level calculations over a logical partition of data without collapsing it like groupBy. Essential for ranking, running totals, moving averages, and deduplication.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "Rank customers by total purchase per month\n",
    "\n",
    "Compute previous or next transaction amounts (lead/lag)\n",
    "\n",
    "Moving averages over last N rows\n",
    "\n",
    "Deduplicate latest records per key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15095cde-6e8f-4bad-80e0-55bbcfc406a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c13ee4-2562-43bc-b5dc-72aa086954b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Rank\n",
    "SELECT CustomerID, OrderDate, Amount,\n",
    "       RANK() OVER (PARTITION BY CustomerID ORDER BY Amount DESC) AS RankAmt\n",
    "FROM Orders;\n",
    "\n",
    "-- Lead / Lag\n",
    "SELECT CustomerID, OrderDate, Amount,\n",
    "       LAG(Amount, 1) OVER (PARTITION BY CustomerID ORDER BY OrderDate) AS PrevAmount,\n",
    "       LEAD(Amount, 1) OVER (PARTITION BY CustomerID ORDER BY OrderDate) AS NextAmount\n",
    "FROM Orders;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07911ea9-4c87-4811-9c99-8dc8f4ffb237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Define a Window spec: partition + order + optional frame.\n",
    "\n",
    "Apply aggregate functions over this window without collapsing rows.\n",
    "\n",
    "Functions: rank(), dense_rank(), row_number(), lag(), lead(), sum().over(window) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e98ee516-46e6-46e1-a247-926963ddc306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79ee923f-e597-4f26-b7ec-317b4603ca0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Window definition\n",
    "window_spec = Window.partitionBy(\"col1\").orderBy(\"col2\")\n",
    "\n",
    "# Using rank\n",
    "df.withColumn(\"rank_col\", F.rank().over(window_spec))\n",
    "\n",
    "# Using lead/lag\n",
    "df.withColumn(\"prev_val\", F.lag(\"metric\", 1).over(window_spec)) \\\n",
    "  .withColumn(\"next_val\", F.lead(\"metric\", 1).over(window_spec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eef3592-6aec-46be-8977-8de487f89655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e6612de-6c25-4ee3-8d2a-1f19e71b6c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", \"2025-01-01\", 100),\n",
    "    (\"Alice\", \"2025-01-02\", 150),\n",
    "    (\"Bob\", \"2025-01-01\", 200),\n",
    "    (\"Bob\", \"2025-01-02\", 50)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Customer\", \"Date\", \"Amount\"])\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define window per customer ordered by date\n",
    "window_spec = Window.partitionBy(\"Customer\").orderBy(\"Date\")\n",
    "\n",
    "# Add rank, lag, lead\n",
    "result = df.withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "           .withColumn(\"prev_amount\", F.lag(\"Amount\", 1).over(window_spec)) \\\n",
    "           .withColumn(\"next_amount\", F.lead(\"Amount\", 1).over(window_spec))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d61f51a-3872-4ca9-83f4-f1b8ec1911e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "+--------+----------+------+----+-----------+-----------+\n",
    "|Customer|      Date|Amount|rank|prev_amount|next_amount|\n",
    "+--------+----------+------+----+-----------+-----------+\n",
    "|Alice   |2025-01-01|   100|   1|       null|        150|\n",
    "|Alice   |2025-01-02|   150|   2|        100|       null|\n",
    "|Bob     |2025-01-01|   200|   1|       null|         50|\n",
    "|Bob     |2025-01-02|    50|   2|        200|       null|\n",
    "+--------+----------+------+----+-----------+-----------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd483b54-0130-48d8-98c4-e3eef142ca50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Compute row number per Customer ordered by Amount descending.\n",
    "\n",
    "Find previous 2 transaction amounts for each Customer.\n",
    "\n",
    "Compute 3-day moving average of Sales per Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b929a974-1b5c-45c5-a76c-6ace3e8d2d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: Bronze transaction table has CustomerID, OrderDate, Amount.\n",
    "\n",
    "Goal: Silver table with latest order per customer and previous order amount.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Define window by CustomerID ordered by OrderDate DESC.\n",
    "\n",
    "Use row_number() to pick latest order.\n",
    "\n",
    "Use lag() to fetch previous order.\n",
    "\n",
    "Write to Silver Delta for downstream analytics.\n",
    "\n",
    "This is commonly used for customer churn prediction, RFM scoring, and deduplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d887c4-c608-446a-9ab3-a79a11ea10d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Time: O(n) rows per partition; Spark distributes partitions across executors.\n",
    "\n",
    "Space: Depends on partition size. Large partitions may cause memory pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbed0bcb-0ff4-4d0c-bba1-88ec43550f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Not partitioning → all data treated as single partition → huge shuffle and memory issues.\n",
    "\n",
    "Forgetting orderBy → ranks/lag/lead meaningless.\n",
    "\n",
    "Using row_number() instead of rank() when duplicates matter.\n",
    "\n",
    "Applying multiple window functions on huge datasets without caching → recomputation cost."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.3 window aggregates",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
