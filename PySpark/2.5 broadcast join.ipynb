{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182b25d9-eadc-49d2-b84c-d1668df30d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Efficiently join a large table with a much smaller reference table by broadcasting the small table to all executors to avoid shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e11e8d02-996f-4cb9-829b-4c7e9d07fc37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44d53bc1-cba3-4e92-8a44-02cb090f92f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT /*+ BROADCAST(customers) */ *\n",
    "FROM orders\n",
    "JOIN customers ON orders.customer_id = customers.id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ddb5203-ec33-4ceb-bfa5-673c15898d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Use pyspark.sql.functions.broadcast(small_df) in .join() or set spark.sql.autoBroadcastJoinThreshold. Spark will copy the small table to each executor and perform map-side join — avoids expensive shuffle of the large table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83b350a-54a5-4797-b460-5391e9a17ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06bde52-9f8e-492c-86e6-c694f3ad0d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = big_df.join(broadcast(small_df), on=join_keys, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc98069f-f189-415f-aea0-81ea18bc9fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a217751-ae24-4837-8e35-ce5cac76b181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "big = spark.range(0, 10_000_000).withColumnRenamed('id','order_id')\n",
    "small = spark.createDataFrame([(1,'A'),(2,'B')], ['id','name'])\n",
    "\n",
    "res = big.join(broadcast(small), big.order_id == small.id, how='inner')\n",
    "# broadcast makes join map-side if small fits memory threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87ce46d9-b5ec-4bda-95ac-6418767cfd27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Broadcast dim_country (200 rows) to join with large events (tens of millions).\n",
    "\n",
    "Force broadcast for currency_codes in a join; measure explain() and spot BroadcastHashJoin.\n",
    "\n",
    "Try joining users with small_lookup both with and without broadcast and compare stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34330344-1442-4914-ace8-6d5a71687537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "You need to enrich streaming click events with geo_ip mapping (small static table ~50k rows). Implement broadcast join in streaming/batch to avoid shuffle. Validate that the small table fits memory; if not, use partitioning or range join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4595bc4-cb10-45e2-b529-c9cce45e4417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Broadcast join: time O(N) map-side, memory cost = size(small_df) per executor. Avoid if small table > executor memory. Ideal when small << available executor RAM (~tens of MBs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2cfc00-7280-43d5-bb2d-a9f79c002077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Broadcasting a table that does not fit executor memory → OOM.\n",
    "\n",
    "Relying on auto-broadcast threshold without checking (use spark.conf or explicit broadcast).\n",
    "\n",
    "Not accounting for serialized size (many columns or wide rows can be bigger than expected)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.5 broadcast join",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
