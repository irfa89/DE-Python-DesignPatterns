{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6b059b-b1bf-4b7a-b015-2f7a39bde02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Aggregates data over one or more columns. Used for summarizing, reporting, and pre-aggregating data.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "Total sales per customer\n",
    "\n",
    "Average session duration per day\n",
    "\n",
    "Maximum/minimum order amount per region\n",
    "\n",
    "Pre-aggregating before writing to Delta/S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fe18e0-77b5-4967-b270-e27b6f796be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8c8b94e-4eb7-4572-b1b5-b722392d1b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL version\n",
    "SELECT CustomerID, SUM(TotalAmount) AS TotalSpent\n",
    "FROM Orders\n",
    "GROUP BY CustomerID;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756d6898-eb1a-49e3-991e-cedfa699a818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "groupBy defines the grouping key(s), and agg specifies aggregate(s) to compute. PySpark allows multiple aggregates and chaining with functions from pyspark.sql.functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9ff41f-6059-4ab1-b7c2-922b453ad3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfcd63bb-91ee-49bd-9b32-bb15b18b936d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Single or multiple aggregates\n",
    "df.groupBy(\"col1\", \"col2\") \\\n",
    "  .agg(\n",
    "      F.sum(\"metric1\").alias(\"sum_metric1\"),\n",
    "      F.avg(\"metric2\").alias(\"avg_metric2\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "074251d0-305f-4405-9292-6247541b3370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2751bb2d-987b-49b5-93bd-b4c9344d9b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", \"2025-01-01\", 100),\n",
    "    (\"Alice\", \"2025-01-02\", 150),\n",
    "    (\"Bob\", \"2025-01-01\", 200),\n",
    "    (\"Bob\", \"2025-01-02\", 50)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Customer\", \"Date\", \"Amount\"])\n",
    "\n",
    "# Aggregate\n",
    "result = df.groupBy(\"Customer\").agg(\n",
    "    F.sum(\"Amount\").alias(\"TotalAmount\"),\n",
    "    F.avg(\"Amount\").alias(\"AvgAmount\")\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f06f532-d195-40c0-ad1f-a5ed70739241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "+--------+-----------+---------+\n",
    "|Customer|TotalAmount|AvgAmount|\n",
    "+--------+-----------+---------+\n",
    "|Alice   |        250|    125.0|\n",
    "|Bob     |        250|    125.0|\n",
    "+--------+-----------+---------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04d03e7-73b0-47e6-9067-e8e2fab53264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Find total and average order amount per Region.\n",
    "\n",
    "Count number of orders per Customer.\n",
    "\n",
    "Find max and min TransactionAmount per Day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd237a94-55d2-4703-bfe0-819e1961f705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: You have a Bronze sales dataset in S3 with CustomerID, OrderDate, Amount. You need to create a Silver aggregate table showing daily total and average sales per customer.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Read Bronze CSV/Parquet.\n",
    "\n",
    "Use groupBy(\"CustomerID\", \"OrderDate\").\n",
    "\n",
    "Compute sum and avg.\n",
    "\n",
    "Write aggregated table to Delta as Silver.\n",
    "\n",
    "This is exactly what a real DE pipeline does daily for reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "978bde2c-9702-4de8-947e-ae90f4e96600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Time: O(n) over the number of rows (Spark distributes groups across nodes).\n",
    "\n",
    "Space: Depends on the number of unique group keys. Wide cardinality → more memory needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eb50b4b-d7ca-4b76-bf20-4ed61cafee93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Forgetting .alias() → unclear column names.\n",
    "\n",
    "Using Python built-ins (e.g., sum) instead of F.sum → Spark cannot optimize.\n",
    "\n",
    "Grouping on too many high-cardinality columns → driver memory issues.\n",
    "\n",
    "Forgetting to cache if the result is reused multiple times."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1 aggregates",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
