{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176c72d8-a84f-4049-9081-9a1df1e758eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Auto Loader simplifies streaming ingestion of files from cloud storage (S3, ADLS, GCS) into Delta tables.\n",
    "\n",
    "Automatically detects new files without scanning the full directory.\n",
    "\n",
    "Supports schema inference and evolution.\n",
    "\n",
    "Handles incremental loads efficiently into Bronze tables.\n",
    "\n",
    "Used for:\n",
    "\n",
    "Streaming raw logs to Bronze\n",
    "\n",
    "Ingesting JSON/CSV/Parquet files as they arrive\n",
    "\n",
    "Preparing incremental datasets for Silver and Gold transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5328c270-ef55-4e52-b184-cfe0e83f86a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0130a144-b61f-4468-b77b-dd64a1c84953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a table to receive files\n",
    "CREATE TABLE bronze USING DELTA LOCATION '/delta/bronze';\n",
    "\n",
    "-- Stream new files into the table\n",
    "COPY INTO bronze\n",
    "FROM 's3://bucket/raw_data/'\n",
    "FILEFORMAT = JSON\n",
    "PATTERN = '*.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6504b46e-9030-432c-8667-065c6c0bc401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Incremental file discovery: Only new files are processed\n",
    "\n",
    "Schema evolution: New columns are detected automatically\n",
    "\n",
    "Streaming ingestion → Delta Bronze table: Foundation for B→S→G pipelines\n",
    "\n",
    "Reusability: Any file-based ingestion pipeline can be automated with Auto Loader for near-real-time ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "065490b2-55a0-44d1-8d72-c6d3494f9820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f501046-1a92-43d2-b253-8d8f42950287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "\n",
    "df_stream = (spark.readStream.format(\"cloudFiles\")\n",
    "             .option(\"cloudFiles.format\", \"json\")       # or csv/parquet\n",
    "             .option(\"cloudFiles.schemaLocation\", \"/delta/schema\")\n",
    "             .load(\"/mnt/raw_data\"))\n",
    "\n",
    "(df_stream.writeStream\n",
    " .format(\"delta\")\n",
    " .option(\"checkpointLocation\", \"/delta/checkpoints/bronze\")\n",
    " .outputMode(\"append\")\n",
    " .table(\"bronze\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ace381db-e439-4b01-bba6-629dcd112572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "583fa24c-bc15-4ffe-b1d2-9f2511a4eaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulated JSON files arriving in S3\n",
    "# File 1: [{\"id\":\"A\", \"amount\":100}]\n",
    "# File 2: [{\"id\":\"B\", \"amount\":50}]\n",
    "\n",
    "df_stream = (spark.readStream.format(\"cloudFiles\")\n",
    "             .option(\"cloudFiles.format\", \"json\")\n",
    "             .option(\"cloudFiles.schemaLocation\", \"/delta/schema\")\n",
    "             .load(\"/mnt/raw_data\"))\n",
    "\n",
    "(df_stream.writeStream\n",
    " .format(\"delta\")\n",
    " .option(\"checkpointLocation\", \"/delta/checkpoints/bronze\")\n",
    " .outputMode(\"append\")\n",
    " .table(\"bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd4744ce-102c-4820-9eaf-2cc22636eda6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step-by-step:**\n",
    "\n",
    "Auto Loader continuously scans /mnt/raw_data\n",
    "\n",
    "Detects new JSON files automatically\n",
    "\n",
    "Appends new rows to Bronze Delta table\n",
    "\n",
    "Maintains checkpoint for exactly-once delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6819e1bc-69f6-49a0-b316-27eff40be11e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Use Auto Loader to stream CSV logs into Bronze table.\n",
    "\n",
    "Simulate schema evolution by adding a new column and verify ingestion works.\n",
    "\n",
    "Stream daily JSON files and verify deduplication before writing to Silver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7b98bd9-31d0-44e1-902f-1a79b110eb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: Healthcare provider receives hourly patient vitals in JSON from multiple devices:\n",
    "\n",
    "Use Auto Loader to stream data into Bronze\n",
    "\n",
    "Transform and clean in Silver (dedup, normalize units)\n",
    "\n",
    "Aggregate for Gold dashboards (daily averages per patient)\n",
    "\n",
    "Maintain incremental, near-real-time updates with exactly-once guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed604caf-2a31-4d33-9f41-0fd3118e31b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Time: O(new_files) → only new files scanned\n",
    "\n",
    "Space: Checkpoint and schema logs are minimal; raw files stored externally\n",
    "\n",
    "Efficient for high-frequency ingestion at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f858cc-9199-43b4-9e7f-79f2bfec42d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Forgetting checkpointLocation → exactly-once guarantees lost\n",
    "\n",
    "Writing in overwrite mode → drops previous data\n",
    "\n",
    "Not handling schema evolution → ingestion errors\n",
    "\n",
    "Pointing Auto Loader to directories with already processed files → duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4476a37a-0345-405d-9e53-c4f635227394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6.6 auto loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
