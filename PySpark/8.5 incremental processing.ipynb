{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8b66c4f-1012-45ce-84be-3394779168b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Incremental processing efficiently processes only the new or changed data since the last ETL run, instead of reprocessing the entire dataset.\n",
    "Use-cases include:\n",
    "\n",
    "Daily ingestion of transactional logs\n",
    "\n",
    "Updating aggregates for only new sales records\n",
    "\n",
    "Slowly changing dimensions with minimal computation\n",
    "\n",
    "This reduces runtime, I/O, and cloud costs significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0bde7a0-2ada-49f2-b72b-f9762346c987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c860c2ec-aec8-48fb-9d36-a74f32028fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM staging_table s\n",
    "WHERE s.updated_at > (SELECT MAX(updated_at) FROM target_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "112bada6-ad50-41d1-9e28-3c30b61d3f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Track a high-watermark (e.g., max timestamp, max ID) from last batch\n",
    "\n",
    "Filter source data to include only rows greater than this watermark\n",
    "\n",
    "Process and append to target\n",
    "\n",
    "Update watermark after successful ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060084ad-7202-4d56-a563-4e74b209e986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aca5209e-2bbf-4e29-85ca-888378918373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read last watermark from target\n",
    "last_watermark = spark.read.parquet(\"/path/to/target\").agg({\"updated_at\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Read incremental data\n",
    "incremental_df = spark.read.parquet(\"/path/to/source\").filter(col(\"updated_at\") > last_watermark)\n",
    "\n",
    "# Process & write\n",
    "incremental_df.write.mode(\"append\").parquet(\"/path/to/target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdecc52a-b1e2-49ed-b0f5-5ccbae773fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc34141d-828b-4871-8aa7-fcfe9e03755e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as Fmax\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Target table\n",
    "target_df = spark.read.parquet(\"/tmp/target_sales\")\n",
    "last_watermark = target_df.agg(Fmax(\"updated_at\")).collect()[0][0]\n",
    "\n",
    "# Source data\n",
    "source_df = spark.read.parquet(\"/tmp/staging_sales\")\n",
    "incremental_df = source_df.filter(col(\"updated_at\") > last_watermark)\n",
    "\n",
    "# Process (e.g., aggregation)\n",
    "agg_df = incremental_df.groupBy(\"product_id\").sum(\"sales_amount\").withColumnRenamed(\"sum(sales_amount)\", \"total_sales\")\n",
    "\n",
    "# Append to target\n",
    "agg_df.write.mode(\"append\").parquet(\"/tmp/target_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26b650b-4d4b-4637-b30d-6df876325805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step-by-step:**\n",
    "\n",
    "Get max updated_at from target\n",
    "\n",
    "Filter source using watermark\n",
    "\n",
    "Apply transformations on incremental data\n",
    "\n",
    "Append results\n",
    "\n",
    "Update watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db1d0e07-5d7f-4571-bc73-c2ad6642bb4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Process only new customer orders since last run.\n",
    "\n",
    "Incrementally update product inventory counts.\n",
    "\n",
    "Compute only the latest user events per day for analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2c0a0bf-15dc-4efd-afa6-125faa0d1b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: A retail platform ingests 100M+ transaction logs daily. Reprocessing all data is expensive.\n",
    "\n",
    "Solution Approach:\n",
    "\n",
    "Maintain updated_at or ingest_time watermark\n",
    "\n",
    "Filter staging/source tables by watermark\n",
    "\n",
    "Aggregate and join only incremental rows\n",
    "\n",
    "Append results to Silver/Gold tables\n",
    "\n",
    "Update watermark in a metadata table for next run\n",
    "\n",
    "Performance tip: Partition source by date to make watermark filter efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca7d7a1-4106-42d2-8f50-8dd17e2fce77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Time: O(N_inc) where N_inc = number of new/changed rows\n",
    "\n",
    "Space: O(N_inc) for intermediate transformations\n",
    "\n",
    "Scales linearly with incremental data, not full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5325402f-a2be-464c-a10e-9dd6659141a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Not updating watermark → reprocesses same data repeatedly\n",
    "\n",
    "Using incorrect filter column → misses or duplicates data\n",
    "\n",
    "Large partition scans → can be slow if not partitioned\n",
    "\n",
    "Watermark based on unreliable timestamps → data gaps"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "8.5 incremental processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
