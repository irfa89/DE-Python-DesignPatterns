{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd1672f-843f-405f-9c81-aab000d52e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "PySpark UDFs can fail on bad or malformed input (e.g., invalid strings, nulls, or unexpected types). Wrapping the logic in try/except prevents entire jobs from failing and allows returning defaults or logging errors.\n",
    "\n",
    "**Use-cases:**\n",
    "\n",
    "Parsing JSON strings in a column\n",
    "\n",
    "Converting strings to numeric types with potential bad formatting\n",
    "\n",
    "Complex business logic where exceptions may occur per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7344d861-e8f4-435e-ba10-d8cb94714845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1083404-1ed0-4bba-bf63-0f60e164a299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    CASE \n",
    "        WHEN TRY_CAST(amount AS INT) IS NULL THEN 0\n",
    "        ELSE CAST(amount AS INT)\n",
    "    END AS safe_amount\n",
    "FROM raw_table;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc13f8a6-eb57-4bdb-a849-a90315a898d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Wrap row-level transformations in a try/except inside a UDF. You can return a default value or log the error without failing the whole job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837ea0d8-0b99-4f95-9fc8-e074e6eb302e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22391ff-7899-4808-b092-55ed9232e4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def safe_parse(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return 0  # default value\n",
    "\n",
    "safe_parse_udf = udf(safe_parse, IntegerType())\n",
    "\n",
    "df_transformed = df.withColumn(\"safe_amount\", safe_parse_udf(\"amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed5e158b-086b-46ed-841e-7225ba12c816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a879f0-c6dc-40d1-a529-c898f02dc523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"100\",), (\"200\",), (\"abc\",), (None,)]\n",
    "df_raw = spark.createDataFrame(data, [\"amount\"])\n",
    "\n",
    "# UDF with try/except\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def safe_parse(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "safe_parse_udf = udf(safe_parse, IntegerType())\n",
    "df_safe = df_raw.withColumn(\"safe_amount\", safe_parse_udf(\"amount\"))\n",
    "\n",
    "df_safe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2350d8f-99d5-4c3a-acad-824124df2e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "+------+-----------+\n",
    "|amount|safe_amount|\n",
    "+------+-----------+\n",
    "|   100|        100|\n",
    "|   200|        200|\n",
    "|   abc|          0|\n",
    "|  null|          0|\n",
    "+------+-----------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabb747c-91f0-42ed-9669-b34cbc6054c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Write a UDF to safely parse float values from strings, returning -1 for invalid values.\n",
    "\n",
    "Parse dates from a string column, returning None if parsing fails.\n",
    "\n",
    "Create a UDF that divides two columns, returning 0 if division by zero occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4220c2-0a13-42fb-9874-910b29fc0ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: You ingest API response data with a price field that may contain non-numeric values or nulls. Build a pipeline that:\n",
    "\n",
    "Reads raw JSON.\n",
    "\n",
    "Safely converts price to float using a UDF with try/except.\n",
    "\n",
    "Adds a price_usd column converting local currency to USD.\n",
    "\n",
    "Aggregates total revenue per day.\n",
    "\n",
    "Writes the result to Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f9b721-c8b9-4723-8577-8886a8c0d6cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "UDFs run row-by-row, slower than native PySpark functions (withColumn, cast)\n",
    "\n",
    "Complexity: O(n) for n rows.\n",
    "\n",
    "Memory: low, unless logging errors extensively per row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d874dc4-2327-4b3f-9ffb-b43edc0bb00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Using Python UDFs unnecessarily — built-in Spark functions are faster.\n",
    "\n",
    "Returning None inconsistently → may break downstream type expectations.\n",
    "\n",
    "Logging inside UDF for every row → can overwhelm driver logs.\n",
    "\n",
    "Forgetting to specify the return type in the UDF."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7.2 error handling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
