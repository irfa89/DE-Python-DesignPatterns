{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec62b52-f6f8-40a8-92bb-a89571c6267b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Captures pipeline execution metadata (success, failure, row counts, timing) for traceability, debugging, and compliance. Helps monitor production pipelines and maintain audit trails.\n",
    "\n",
    "Use-cases:\n",
    "\n",
    "Recording the number of rows processed per stage\n",
    "\n",
    "Capturing pipeline start/end timestamps\n",
    "\n",
    "Logging errors and exceptions\n",
    "\n",
    "Maintaining audit tables for regulatory or internal reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ee8458-85d3-4ac0-988b-5f7f7954e5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "100f3661-c218-48eb-8fc5-d46326f66323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Insert execution log\n",
    "INSERT INTO pipeline_audit\n",
    "(pipeline_name, run_id, start_time, end_time, status, row_count)\n",
    "VALUES ('customer_pipeline', 'run_20251204', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, 'SUCCESS', 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab2a9665-140b-4848-b1a0-48ddf2a781e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Maintain a central audit table and log key metrics at each step. Use PySpark to calculate metrics, and write to Delta, Parquet, or external monitoring tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff2d2c4-071e-4960-b5d8-2861d3568b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4de228-507c-4771-ad1d-b6c9d016c6fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "row_count = df.count()\n",
    "\n",
    "status = \"SUCCESS\"\n",
    "\n",
    "audit_record = spark.createDataFrame([\n",
    "    [pipeline_name, run_id, start_time, datetime.now(), status, row_count]\n",
    "], [\"pipeline_name\", \"run_id\", \"start_time\", \"end_time\", \"status\", \"row_count\"])\n",
    "\n",
    "audit_record.write.format(\"delta\").mode(\"append\").save(\"/path/to/audit_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b1c8713-e8b5-4d33-97fe-c75d6ce60a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa30f995-7304-4829-9a8f-9b62782dd12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "pipeline_name = \"customer_pipeline\"\n",
    "run_id = \"run_20251204\"\n",
    "\n",
    "data = [(1, \"Alice\"), (2, \"Bob\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "start_time = datetime.now()\n",
    "row_count = df.count()\n",
    "\n",
    "status = \"SUCCESS\"\n",
    "\n",
    "audit_record = spark.createDataFrame([\n",
    "    [pipeline_name, run_id, start_time, datetime.now(), status, row_count]\n",
    "], [\"pipeline_name\", \"run_id\", \"start_time\", \"end_time\", \"status\", \"row_count\"])\n",
    "\n",
    "audit_record.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f04fadd-8786-4a1b-85f1-bb6d5ca87972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "+----------------+----------+-------------------+-------------------+-------+---------+\n",
    "|   pipeline_name|    run_id|         start_time|           end_time| status|row_count|\n",
    "+----------------+----------+-------------------+-------------------+-------+---------+\n",
    "|customer_pipeline|run_20251204|2025-12-04 15:00:00|2025-12-04 15:00:05|SUCCESS|        2|\n",
    "+----------------+----------+-------------------+-------------------+-------+---------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ceaec8-b63a-40ef-a284-a5b182f121c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Log row counts for multiple DataFrames in a single run.\n",
    "\n",
    "Add a column to capture the environment (dev, prod) in audit logs.\n",
    "\n",
    "Log pipeline failures using try/except and store status as \"FAILURE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02138c2e-707f-4533-a069-bed711e08a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: You run a nightly ETL pipeline:\n",
    "\n",
    "Read multiple raw tables → clean and transform.\n",
    "\n",
    "Count rows processed per table.\n",
    "\n",
    "Capture start/end times, environment, run ID, and error messages.\n",
    "\n",
    "Write audit logs to a Delta table.\n",
    "\n",
    "Send alert email if any table fails QC checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89d6846c-30ae-439b-8d05-75a65da0e33c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "Counting rows → triggers a Spark job (O(n) per DataFrame).\n",
    "\n",
    "Logging itself is minimal memory.\n",
    "\n",
    "Avoid writing audit logs inside every transformation; batch logging after major steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69f123fd-3ba4-4ac3-bfbd-c67dd49c201c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Logging after .collect() on huge DataFrames → memory explosion.\n",
    "\n",
    "Not including run_id or timestamp → difficult to trace executions.\n",
    "\n",
    "Overlogging every row → bloats audit table.\n",
    "\n",
    "Ignoring failures → pipeline may fail silently without proper audit."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7.4 logging and audit pattern",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
