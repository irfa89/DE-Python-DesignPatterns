{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ee77c20-2711-444f-af18-4de836c25f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 1. What This Pattern Solves**\n",
    "\n",
    "Reduces expensive shuffles in Spark jobs by minimizing wide transformations (like groupBy, join, distinct, repartition).\n",
    "\n",
    "Wide transformations require data movement across partitions → slow and resource-heavy.\n",
    "\n",
    "Narrow transformations (like map, filter, select) operate within partitions → fast.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "Large joins in ETL pipelines.\n",
    "\n",
    "Aggregations over billions of rows.\n",
    "\n",
    "Improving streaming pipeline latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b037e96-e2df-46c4-80b0-2b36d945cd7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 2. SQL Equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe5a7a3-0cf0-4779-b2ce-6ebcd8c45abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Wide transformation (shuffle-heavy)\n",
    "SELECT user_id, SUM(amount)\n",
    "FROM transactions\n",
    "GROUP BY user_id;\n",
    "\n",
    "-- Narrow transformations (no shuffle)\n",
    "SELECT user_id, amount * 2 AS double_amount\n",
    "FROM transactions\n",
    "WHERE amount > 100;\n",
    "\n",
    "-- Wide = requires redistribution.\n",
    "-- Narrow = per-row operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c90e8a-08c3-4c18-945c-586cdf936dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 3. Core Idea**\n",
    "\n",
    "Narrow transformations → partition-local → low cost.\n",
    "\n",
    "Wide transformations → require shuffle → optimize by:\n",
    "\n",
    "Reducing data before shuffle (filter, select).\n",
    "\n",
    "Using partitioning wisely (repartition/broadcast join).\n",
    "\n",
    "Avoiding unnecessary wide operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4314e8a-22ca-4078-a7cc-1ceab6fb1dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 4. Template Code (MEMORIZE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d699cc7-17f8-48b4-9a38-2a16f38389dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Narrow transformations\n",
    "df2 = df.filter(\"amount > 100\").select(\"user_id\", \"amount\")\n",
    "\n",
    "# Wide transformations\n",
    "agg_df = df2.groupBy(\"user_id\").sum(\"amount\")  # causes shuffle\n",
    "\n",
    "# Optimization: pre-aggregate before join or reduce data\n",
    "small_df = small_df.select(\"user_id\", \"value\")\n",
    "joined_df = df2.join(broadcast(small_df), \"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4441acee-8239-49b6-afb7-00b6d8d53050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 5. Detailed Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5bf2ba-f961-4c07-b6e9-545346f32fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PerfPatterns\").getOrCreate()\n",
    "data = [(\"Alice\", 100), (\"Bob\", 200), (\"Charlie\", 300)]\n",
    "df = spark.createDataFrame(data, [\"user\", \"amount\"])\n",
    "\n",
    "# Narrow transformation\n",
    "df_narrow = df.filter(col(\"amount\") > 100).select(\"user\", \"amount\")\n",
    "\n",
    "# Wide transformation (groupBy triggers shuffle)\n",
    "agg_df = df_narrow.groupBy(\"user\").agg(spark_sum(\"amount\").alias(\"total\"))\n",
    "\n",
    "# Optimize join by broadcasting small DF\n",
    "small_df = spark.createDataFrame([(\"Alice\", \"NY\")], [\"user\", \"state\"])\n",
    "joined_df = df_narrow.join(broadcast(small_df), \"user\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eeb3c63-0dde-4332-8048-0725ffd02f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "filter + select = narrow → fast.\n",
    "\n",
    "groupBy = wide → triggers shuffle.\n",
    "\n",
    "broadcast avoids shuffle in small-large joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b72397-0030-4c4c-8ace-9f93931354da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 6. Mini Practice Problems**\n",
    "\n",
    "Identify wide vs narrow transformations in this code: df.filter(...).join(...).select(...).\n",
    "\n",
    "How would you optimize a groupBy on a massive DataFrame?\n",
    "\n",
    "When is broadcasting a small table helpful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531e18eb-aa05-4a57-bb94-c1dd63987055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 7. Full Data Engineering Problem**\n",
    "\n",
    "Scenario: A 500M row clickstream dataset is joined with a 1000-row user profile table. After filtering clicks from the last week, we want aggregated metrics per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96579916-2f33-4a0d-b4d3-5e6158405f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, broadcast\n",
    "\n",
    "# Step 1: Narrow transformation: filter\n",
    "df_filtered = clicks.filter(col(\"event_date\") >= \"2025-12-01\")\n",
    "\n",
    "# Step 2: Optimize join with broadcast\n",
    "df_joined = df_filtered.join(broadcast(user_profile), \"user_id\")\n",
    "\n",
    "# Step 3: Wide transformation: groupBy\n",
    "agg_df = df_joined.groupBy(\"user_id\").agg(spark_sum(\"clicks\").alias(\"total_clicks\"))\n",
    "agg_df.show()\n",
    "\n",
    "## Minimizes shuffle while performing wide operations only after reducing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e99cde-d075-4926-ad61-5ab2dd7ab94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 8. Time & Space Complexity**\n",
    "\n",
    "| Operation               | Time Complexity      | Space Complexity      |\n",
    "| ----------------------- | -------------------- | --------------------- |\n",
    "| Narrow (filter, select) | O(n)                 | O(n)                  |\n",
    "| Wide (groupBy, join)    | O(n log p + shuffle) | O(n + shuffle buffer) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bf4f1c5-59d3-4abc-a02c-670c8a8bbaef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⭐ 9. Common Pitfalls**\n",
    "\n",
    "Applying wide transformations too early → massive shuffle.\n",
    "\n",
    "Joining large tables without broadcasting → unnecessary data movement.\n",
    "\n",
    "Forgetting to pre-filter → shuffling unnecessary data.\n",
    "\n",
    "Ignoring partitioning → uneven partition sizes → skew."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.4 avoiding wide transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
